{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wUsAGTPUcnv"
   },
   "source": [
    "### <img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales - 2019-2 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 2  </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Manipulaciones en pandas y numpy, preprocesamientos\n",
    "* Recurrent Neural Networks \n",
    "* LSTM, GRU\n",
    "* Autoencoders\n",
    "* GAN\n",
    " \n",
    "\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de 2 personas (*Ambos estudiantes deben estar preparados para presentar la tarea el día de la entrega*).\n",
    "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos), se penalizará fuertemente ausencia de comentarios, explicaciones de gráficos, _etc_. Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
    "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
    "* Formato de entrega: envı́o de link del repositorio en _Github_, al correo electrónico del ayudante (<alvaro.valderrama.13@sansano.usm.cl>), en copia al profesor (<cvalle@inf.utfsm.cl>).   Especificar el siguiente asunto: [INF-395/477-2019 Tarea 2]. Invitar como colaborador al usuario de github \"avalderr\" para poder acceder al repositorio en caso de ser privado.\n",
    "* Fecha de entrega y presentaciones: 21 de Febrero 2020 (Fase I). Hora límite de entrega: 23:59. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail igualmente.  \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en tres partes:\n",
    "\n",
    "* 1 - Redes Recurrentes en Texto\n",
    "* 2 - Autoencoders en Fashion MNIST    \n",
    "* 3 - GAN para MNIST.\n",
    "\n",
    "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo, solo son guias y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
    "Recuerden intercalar su código con *comentarios* en celdas _Markdown_, con los comentarios de la pregunta y con cualquier analisis, fórmula (en $ \\LaTeX $) o explicación que les parezca relevante para justificar sus procedimientos. *No respondan las preguntas en comentarios en el código*.\n",
    "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará tanto la elección en si. En cambio la argumentación detrás de la elección será lo más ponderado.\n",
    "Si algun modelo se demora demasiado en correr en su maquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, incluso con la opción de aceleración con GPU (particularmente útil para los modelos más grandes), esto puede ser relevante para las maquinas más lentas al momento de realizar exploraciones con _K-folds_ o las redes más grandes. Existe también la posibilidad de utilizar _Google Cloud Plataform_, donde tienen 300 dolares de prueba por un año y pueden comprar tiempo de procesamiento en maquinas aceleradas con GPU; maquinas ya configuradas para _deep leraning_ pueden encontrarse en el _Marketplace_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cXszO_JUcnz"
   },
   "source": [
    "# 1 - Redes Recurrentes en texto\n",
    "\n",
    "El analisis de texto y en general de lenguaje natural es una de las tareas desafiantes que los avances tanto del machine learning como de redes neuronales han logrado potenciar el los último años. La naturaleza inherentemente secuencial del texto y sus características similares a variables categóricas han inspirado muchos avances interesantes en el area. Para esta primera aproximación a procesamiento de secuencias, y en particular de texto, utilizaremos un dataset relativamente estandar, de texto anotado con categorias de entidades. Este dataset se encuentra disponible en el siguiente link https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus. \n",
    "\n",
    "Durante esta pregunta realizaremos dos tareas distintas, la de predicción del _tag_ para cada palabra de la secuencia (_many to many_) y la de predicción del siguiente caracter (_many to one_). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbP0duAXUcn0"
   },
   "source": [
    "#### 1.a El dataset\n",
    "\n",
    "Investigue en la documentación del dataset cual es la tarea original para el cual fue propuesto, en particular cual es la variable que buscamos predecir, a qué se refiere esta misma y por qué es necesario utilizar técnicas avanzadas para resolver esta tarea (¿no bastaría con un diccionario? De un ejemplo en caso contrario). \n",
    "\n",
    "Cargue el conjunto de datos. Este conjunto de datos es bastante grande, por lo que como ven en el código propuesto, nos contentaremos con no considerar las lineas corruptas del registro.\n",
    "\n",
    "Solo nos quedaremos con `word` y `lemma` de las variables regresoras, guardando `sentence_idx` para poder reconstruir las frases como unidad básica con la que trabajaremos. ¿Qué diferencia hay entre la palabra y su lemma? De ejemplos donde la diferencia sea significativa y explique en qué circunstancias preferiría una sobra la otra. \n",
    "\n",
    "Para la primera parte de estas preguntas, preferiremos los lemas pues buscaremos predecir el `tag` de la palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gc5Gfd_SadSj",
    "outputId": "b904de74-8e3d-4b03-f0dc-2c6ee47d231f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "# Colab library to upload files to notebook\n",
    "from google.colab import files\n",
    "\n",
    "# Install Kaggle library\n",
    "!pip install -q kaggle\n",
    "\n",
    "# check if dataset exist\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgrHw_4UagfU"
   },
   "outputs": [],
   "source": [
    "!mkdir /root/.kaggle\n",
    "!echo '{\"username\":\"sebastingallardo\",\"key\":\"df9d642f8dce03217fe5b41f25881f75\"}' > /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "QA9G8w4Maq5x",
    "outputId": "de74ff70-03c6-48e9-efbc-917d22b04521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading entity-annotated-corpus.zip to /content\n",
      "\r",
      "  0% 0.00/26.4M [00:00<?, ?B/s]\r",
      " 19% 5.00M/26.4M [00:00<00:01, 20.7MB/s]\n",
      "\r",
      "100% 26.4M/26.4M [00:00<00:00, 76.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8DdExj9eRVS"
   },
   "outputs": [],
   "source": [
    "!unzip -q 'entity-annotated-corpus.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IufsyvdAen0o",
    "outputId": "67df97e1-b7a2-48f1-aeb8-548193b7b100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "['.config', 'entity-annotated-corpus.zip', 'ner.csv', 'ner_dataset.csv', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "print(os.getcwd())\n",
    "data_path = os.path.join(os.getcwd())\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IKAOYucqUcn1",
    "outputId": "9ea78699-be83-46fc-9989-a29887ce0648"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 281837: Expected 25 fields in line 281837, saw 34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(\"ner.csv\"), engine='python', error_bad_lines=False)\n",
    "df = df.dropna()[['lemma','tag','word','sentence_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "zauuzv2Dfprm",
    "outputId": "ac86febe-6cb3-4e35-a338-3ddbb112583d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lemma tag           word  sentence_idx\n",
      "0        thousand   O      Thousands           1.0\n",
      "1              of   O             of           1.0\n",
      "2        demonstr   O  demonstrators           1.0\n",
      "3            have   O           have           1.0\n",
      "4           march   O        marched           1.0\n",
      "...           ...  ..            ...           ...\n",
      "1050790      they   O           they       47959.0\n",
      "1050791   respond   O      responded       47959.0\n",
      "1050792        to   O             to       47959.0\n",
      "1050793       the   O            the       47959.0\n",
      "1050794    attack   O         attack       47959.0\n",
      "\n",
      "[1050794 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-zFeEzWUcn7"
   },
   "source": [
    "#### 1.b Preprocesamiento\n",
    "\n",
    "Para poder utilizar este conjunto de datos, debemos transformar nuestra tabla de palabras y sentencias, a una tabla donde cada entrada sea una sentencia, ademas codificando los distintos lemmas y tags como valores numericos. Esto pueden realizarlo con alguna de las utilidades de `keras` o `sklearn`, sin embargo en el código siguiente se propone un metodo solo usando python y `pandas`. Pueden utilizar el método que deseen. Note eso si que independiente la aproximación que utilice debe comenzar desde 1 para la codificación, pues el valor 0 lo reservaremos para representar la ausencia de palabras más adelante. \n",
    "\n",
    "Complete y explique que realiza cada linea del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7k-o_xmGUcn9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lemma_to_code = {lemma:code+1 for code, lemma in enumerate(df.lemma.unique())}\n",
    "tag_to_code = {tag:code+1 for code, tag in enumerate(df.tag.unique())}\n",
    "n_lemmas = len(lemma_to_code)\n",
    "\n",
    "df['lemma'] = df.lemma.apply(lambda x: lemma_to_code[x])\n",
    "df['tag'] = df.tag.apply(lambda x: tag_to_code[x])\n",
    "\n",
    "dff = df.groupby(\"sentence_idx\")[['lemma','tag']].agg(list).applymap(np.asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7OQFSWyrICy5",
    "outputId": "a2358adb-748a-4b54-af60-10536094c543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'B-geo': 2, 'B-gpe': 3, 'B-per': 4, 'I-geo': 5, 'B-org': 6, 'I-org': 7, 'B-tim': 8, 'B-art': 9, 'I-art': 10, 'I-per': 11, 'I-gpe': 12, 'I-tim': 13, 'B-nat': 14, 'B-eve': 15, 'I-eve': 16, 'I-nat': 17}\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(tag_to_code)\n",
    "tag_to_code[\"PAD\"] = 0\n",
    "n_tags = len(tag_to_code)\n",
    "print(n_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uygAlSB0UcoB"
   },
   "source": [
    "#### 1.c Padding\n",
    "\n",
    "Ahora que ya tenemos las sentencias codificadas y agrupadas, explore el tamaño de estas, en número de lemmas: ¿Son todas las sentencias de igual tamaño? ¿Le hace sentido esto? ¿Las redes que conoce pueden manajar ejemplos de distintos tamaños, y si pueden que problemas podría traer? ¿Estan las clases repartidas de manera equitativa?\n",
    "\n",
    "En esta parte de la tarea, deben lograr que todas las secuencias de lemmas (y los tags correspondientes) queden del mismo largo, es decir realizar _padding_. El _padding_ debe realizarse con el valor 0, pueden escoger si realizarlo al comienzo de la secuencia o al final, expliquen su elección. Pueden utilizar la función `keras.preprocessing.sequence.pad_sequences` o escribir sus propios códigos. Elija un valor de `maxlen` que le parezca adecuado.\n",
    "\n",
    "¿Opinan que es deseable utilizar el valor 0 como codificación de palabras que \"no existen\", o creen que es irrelevante por ejemplo que su valor sea 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ESviEzbd9a9t",
    "outputId": "1a38488b-1219-4206-ed2e-1a4aaf3df16c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35177, 2)\n",
      "                                                          lemma                                                tag\n",
      "sentence_idx                                                                                                      \n",
      "1.0           [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, ...\n",
      "2.0           [23, 2, 24, 25, 12, 10, 26, 27, 10, 9, 28, 29,...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "3.0           [42, 5, 19, 10, 43, 2, 44, 8, 45, 46, 12, 47, ...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, ...\n",
      "4.0           [49, 50, 10, 37, 2, 51, 52, 53, 54, 55, 56, 57...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "5.0           [10, 9, 60, 61, 10, 62, 2, 10, 63, 64, 2, 65, ...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 6, ...\n",
      "...                                                         ...                                                ...\n",
      "47955.0       [1261, 489, 148, 308, 172, 716, 233, 484, 1360...  [3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "47956.0       [1261, 119, 164, 356, 38, 58, 841, 12, 334, 66...  [3, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, ...\n",
      "47957.0       [175, 277, 235, 12, 205, 2326, 8, 45, 817, 719...                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "47958.0       [42, 120, 257, 612, 2, 10, 1110, 2255, 2638, 4...                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "47959.0                   [1261, 308, 164, 42, 104, 8, 10, 214]                           [3, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "[35177 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dff.shape)\n",
    "print(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "S3TP_rlLrUZN",
    "outputId": "89beb434-0caa-4ecd-f498-e1f113b9cb6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "(70,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x7fb8ff128278>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fb8ff129eb8>,\n",
       "  <matplotlib.lines.Line2D at 0x7fb8ff1296a0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fb8ff12c4e0>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fb8ff129cf8>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x7fb8ff128e10>,\n",
       "  <matplotlib.lines.Line2D at 0x7fb8ff1297b8>]}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(dff.iloc[4].lemma.shape)\n",
    "print(dff.iloc[42].lemma.shape)\n",
    "largo = []\n",
    "for i in range(dff.shape[0]):\n",
    "  largo.append(dff.iloc[i].lemma.shape[0])\n",
    "\n",
    "plt.boxplot(largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cekxdNpGA1Fx",
    "outputId": "b8d1946c-9a52-469a-ebd3-11cf98ddffd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print(dff.iloc[423].tag.shape == dff.iloc[423].lemma.shape)\n",
    "print(dff.iloc[34242].tag.shape == dff.iloc[34242].lemma.shape)\n",
    "print(np.max(largo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "8JfDPL4cUcoC",
    "outputId": "7edb5aaf-6d9d-4eab-9c1e-9546f16df3e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 1 1]\n",
      " [0 0 0 ... 1 1 1]\n",
      " [0 0 0 ... 2 5 1]\n",
      " ...\n",
      " [0 0 0 ... 1 1 1]\n",
      " [0 0 0 ... 1 1 1]\n",
      " [0 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 140\n",
    "x = pad_sequences(dff.lemma.values,\n",
    "                  maxlen=max_len,\n",
    "                  padding='pre',\n",
    "                  truncating='post'\n",
    ")\n",
    "\n",
    "y = pad_sequences(dff.tag.values,\n",
    "                  maxlen=max_len,\n",
    "                  padding='pre',\n",
    "                  truncating='post'\n",
    ")\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azMxxxVMUcoF"
   },
   "source": [
    "#### 1.d Even more encodding and preprocessing\n",
    "\n",
    "Para la primera tarea, buscaremos realizar la predicción del tag, la cual es una variable categórica. Por lo tanto, como ya sabemos de la tarea anterior, resulta conveniente tener los _targets_ en _one hot vector_. \n",
    "\n",
    "Además debemos separar ambos conjuntos (`x` e `y`) en los conjuntos de entrenamiento y validación. \n",
    "\n",
    "Realice esto con las herramientas y proporciones que estime convenientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PbY34nCWUcoH",
    "outputId": "c1598a1f-c389-4134-a19e-fb3f660034e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26382, 140)\n",
      "(8795, 140)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "y_tr = np.array(y_tr)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(x_tr.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbFc1vzkUcoL"
   },
   "source": [
    "#### 1.e Primera red recurrente\n",
    "\n",
    "Ahora entrenaremos una primera red recurrente LSTM. Explique la particularidad de estas redes y por qué podría comportarse bien para este tipo de problemas. \n",
    "\n",
    "La tarea de esta red será procesar la secuencia de _lemmas_ y predecir su _tag_. ¿Cuáles son las dimensiones de nuestro input y de nuestro output?\n",
    "\n",
    "Considerando que si bien los _lemmas_ estan representados como enteros, esta representación no tiene ninguna relación con el \"significado\" de estos. Por eso, es recomendable utilizar otra representación que mejor extraiga la naturaleza \"categorica\" de estos, sin embargo por la cantidad de ejemplos que tenemos, no resultaría practico utilizar _one hot vector_, pues las dimensiones explotarían. Para esto, utilizaremos una primera capa que realiza una transformación entrenable entre el espacio discreto de la representación original y un _embedding_ en $\\mathbb{R}^d$ donde $d$ corresponde a la dimensión de nuestro espacio de llegada.\n",
    "\n",
    "Utilice un _embedding_ de dmensión 100, luego agregue una capa LSTM con 128 unidades y finalmente la capa densa de salida. Compile y entrene el modelo por al menos 10 epochs. Grafique el proceso de entrenamiento y evalue el desempeño final considerando alguna métrica relevante (F1 score, matriz de confusión, etc), considerando la repartición de clases que observó en la pregunta c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "_E8Z9JaAUcoM",
    "outputId": "6fe2977c-88e2-47f4-e440-6ec57533feef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 140, 128)          117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 2,143,470\n",
      "Trainable params: 2,143,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.3948 - acc: 0.9246 - val_loss: 0.1644 - val_acc: 0.9670\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.1238 - acc: 0.9691 - val_loss: 0.0833 - val_acc: 0.9757\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0658 - acc: 0.9821 - val_loss: 0.0544 - val_acc: 0.9860\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0442 - acc: 0.9883 - val_loss: 0.0431 - val_acc: 0.9882\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0351 - acc: 0.9901 - val_loss: 0.0395 - val_acc: 0.9887\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0309 - acc: 0.9908 - val_loss: 0.0373 - val_acc: 0.9890\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0282 - acc: 0.9913 - val_loss: 0.0374 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0263 - acc: 0.9917 - val_loss: 0.0364 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0248 - acc: 0.9921 - val_loss: 0.0360 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0235 - acc: 0.9925 - val_loss: 0.0371 - val_acc: 0.9894\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "ltsm = Sequential()\n",
    "\n",
    "embedding_dim = 100\n",
    "ltsm.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "ltsm.add(LSTM(units=128,return_sequences=True)) # You can use CuDNNLSTM if you have a CUDA enabled GPU for faster performance\n",
    "\n",
    "ltsm.add(Dense(n_tags, activation='softmax'))\n",
    "\n",
    "ltsm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "ltsm.summary()\n",
    "history = ltsm.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "q6vGWkKbOSDA",
    "outputId": "9663322e-603f-4079-f0e8-189bd008ac95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.03737807533258467\n",
      "Test accuracy: 0.9892203361219544\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(history.history[\"acc\"],label=\"training accuracy\")\n",
    "plt.plot(history.history[\"val_acc\"],label=\"val accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot\n",
    "score = ltsm.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "uem9SFea_JsM",
    "outputId": "9c65af0c-2948-4c3a-af74-98dbbc31454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.5)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=4d9603e219bece2f94f4ad2f9430a83ff87436af32ce5c4ddcb5950ff1cb0e75\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-0.0.12\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.28.1)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
      "\u001b[K     |████████████████████████████████| 757kB 7.6MB/s \n",
      "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00xjsiRigypw"
   },
   "outputs": [],
   "source": [
    "idx2tag = {i: w for w, i in tag_to_code.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(list(idx2tag.values())[p_i].replace(\"PAD\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EY2w3ZhF_-nB",
    "outputId": "92a0cd4c-9f82-4083-e4e8-d19d7a80ec38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8795/8795 [==============================] - 20s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = ltsm.predict(x_val, verbose=1)   \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BJx_weC81i3D",
    "outputId": "4f7bcff2-5b3e-4fde-99c7-7b2b0846b429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 94.7%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from  sklearn_crfsuite.metrics import flat_classification_report  \n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "k-RJScSnVW4T",
    "outputId": "eb7b5f0d-fea8-4cd5-d82b-14c224899e84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.91      0.80      0.85      5000\n",
      "       B-eve       0.00      0.00      0.00        66\n",
      "       B-geo       0.97      0.99      0.98    220977\n",
      "       B-gpe       0.81      0.81      0.81      9380\n",
      "       B-nat       0.79      0.57      0.66      1494\n",
      "       B-org       0.79      0.66      0.72      1828\n",
      "       B-per       0.90      0.91      0.91      4145\n",
      "       B-tim       0.66      0.53      0.59      4104\n",
      "       I-art       0.00      0.00      0.00       100\n",
      "       I-eve       0.71      0.05      0.10        96\n",
      "       I-geo       0.81      0.71      0.75      4058\n",
      "       I-gpe       0.86      0.81      0.84      4134\n",
      "       I-nat       0.00      0.00      0.00        83\n",
      "       I-org       0.70      0.49      0.58      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.00      0.00      0.00        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.58      0.49      0.52   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_b3LMn57UcoQ"
   },
   "source": [
    "#### 1.f Comparación con otras aproximaciones. \n",
    "\n",
    "Ahora entrenaremos dos redes más para comparar los desempeños de la red recurrente. Para esto entrenaremos una red convolucional 1-dimensional y una red densa. Discuta las diferencias entre ambas redes y la red recurrente, y cual cree a priori se adecua mejor a la naturaleza del problema. \n",
    "\n",
    "Defina y entrene luego las redes con la misma primera capa de encodding y las siguientes configuraciones:\n",
    "* Convolucional: 2 capas conv1d con 128 filtros y kernel de tamaño 5\n",
    "* Densa: 2 capas densas de 128 unidades\n",
    "\n",
    "Compare los desempeños de las 3 redes en el problema su métrica preferida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "WIFrmqY7UcoR",
    "outputId": "f26f9cd6-7281-4394-f8b7-59087e8ee273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 140, 128)          64128     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 2,172,398\n",
      "Trainable params: 2,172,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 3s 116us/step - loss: 0.2401 - acc: 0.9675 - val_loss: 0.0683 - val_acc: 0.9792\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0499 - acc: 0.9866 - val_loss: 0.0401 - val_acc: 0.9893\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0286 - acc: 0.9919 - val_loss: 0.0328 - val_acc: 0.9907\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0320 - val_acc: 0.9906\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0177 - acc: 0.9945 - val_loss: 0.0323 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0335 - val_acc: 0.9908\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0350 - val_acc: 0.9905\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0373 - val_acc: 0.9904\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0401 - val_acc: 0.9902\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0422 - val_acc: 0.9904\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "conv = Sequential()\n",
    "\n",
    "embedding_dim = 100\n",
    "conv.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "conv.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
    "conv.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
    "conv.add(Dense(n_tags, activation=\"softmax\"))\n",
    "\n",
    "conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = conv.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=0, batch_size=128)\n",
    "conv.summary()\n",
    "history = conv.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "i1qUI8EsKUfo",
    "outputId": "6d26ca86-efaf-42bb-c569-10cd9f616abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8795/8795 [==============================] - 0s 49us/step\n",
      "F1-score: 95.3%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from  sklearn_crfsuite.metrics import flat_classification_report  \n",
    "\n",
    "test_pred = conv.predict(x_val, verbose=1)   \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_val)\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "7IXrirSjK0DA",
    "outputId": "065c5a65-2577-42ba-b2d5-632cebbc316f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.89      0.86      0.88      5000\n",
      "       B-eve       0.57      0.12      0.20        66\n",
      "       B-geo       0.98      0.99      0.98    220977\n",
      "       B-gpe       0.83      0.83      0.83      9380\n",
      "       B-nat       0.71      0.71      0.71      1494\n",
      "       B-org       0.80      0.68      0.74      1828\n",
      "       B-per       0.94      0.90      0.92      4145\n",
      "       B-tim       0.69      0.55      0.61      4104\n",
      "       I-art       0.39      0.14      0.21       100\n",
      "       I-eve       0.57      0.22      0.32        96\n",
      "       I-geo       0.82      0.76      0.79      4058\n",
      "       I-gpe       0.83      0.82      0.83      4134\n",
      "       I-nat       0.47      0.11      0.18        83\n",
      "       I-org       0.72      0.59      0.64      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.96      0.46      0.62        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.72      0.57      0.61   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "sLNSZDe69G1_",
    "outputId": "48c9ca3c-cce7-4f46-84f9-17c89d76551e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 140, 128)          12928     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 140, 128)          16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 2,055,662\n",
      "Trainable params: 2,055,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 2s 90us/step - loss: 0.4261 - acc: 0.9623 - val_loss: 0.0604 - val_acc: 0.9845\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 1s 56us/step - loss: 0.0504 - acc: 0.9859 - val_loss: 0.0496 - val_acc: 0.9860\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 1s 55us/step - loss: 0.0432 - acc: 0.9872 - val_loss: 0.0484 - val_acc: 0.9861\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 1s 55us/step - loss: 0.0409 - acc: 0.9875 - val_loss: 0.0484 - val_acc: 0.9860\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 2s 57us/step - loss: 0.0398 - acc: 0.9876 - val_loss: 0.0483 - val_acc: 0.9861\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 1s 56us/step - loss: 0.0392 - acc: 0.9876 - val_loss: 0.0483 - val_acc: 0.9862\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 1s 55us/step - loss: 0.0387 - acc: 0.9876 - val_loss: 0.0483 - val_acc: 0.9863\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 2s 58us/step - loss: 0.0384 - acc: 0.9877 - val_loss: 0.0484 - val_acc: 0.9861\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 2s 57us/step - loss: 0.0381 - acc: 0.9877 - val_loss: 0.0486 - val_acc: 0.9862\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 1s 56us/step - loss: 0.0378 - acc: 0.9877 - val_loss: 0.0487 - val_acc: 0.9862\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "dense = Sequential()\n",
    "\n",
    "embedding_dim = 100\n",
    "dense.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "dense.add(Dense(128, activation='relu'))\n",
    "dense.add(Dense(128, activation='relu'))\n",
    "dense.add(Dense(n_tags, activation=\"softmax\"))\n",
    "\n",
    "dense.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = dense.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=0, batch_size=128)\n",
    "dense.summary()\n",
    "history = dense.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "J9j0XrT3LBBH",
    "outputId": "85080c10-6de1-4511-944a-d5eeb4dd11b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8795/8795 [==============================] - 1s 71us/step\n",
      "F1-score: 93.2%\n"
     ]
    }
   ],
   "source": [
    "test_pred = dense.predict(x_val, verbose=1)   \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_val)\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "iGgUX6SJLRic",
    "outputId": "5c93f3fe-9e6f-4779-8ed8-d34e9b3d0898"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.86      0.79      0.82      5000\n",
      "       B-eve       0.67      0.15      0.25        66\n",
      "       B-geo       0.96      0.99      0.98    220977\n",
      "       B-gpe       0.77      0.81      0.79      9380\n",
      "       B-nat       0.64      0.11      0.19      1494\n",
      "       B-org       0.55      0.52      0.54      1828\n",
      "       B-per       0.93      0.91      0.92      4145\n",
      "       B-tim       0.54      0.17      0.25      4104\n",
      "       I-art       0.50      0.22      0.31       100\n",
      "       I-eve       0.67      0.17      0.27        96\n",
      "       I-geo       0.70      0.63      0.66      4058\n",
      "       I-gpe       0.71      0.67      0.69      4134\n",
      "       I-nat       0.00      0.00      0.00        83\n",
      "       I-org       0.73      0.39      0.51      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.49      0.44      0.46        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.63      0.47      0.51   1231300\n",
      "weighted avg       0.98      0.99      0.98   1231300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zc8HZvirUcoV"
   },
   "source": [
    "#### 1.g Red más profunda o bidireccional\n",
    "\n",
    "Una pregunta natural que surge es saber que ocurre al aumentar la profundidad de la red, o por ejemplo porqué solo se recorre el _input_ en la dirección original. Considerando el tiempo acotado de este semestre, pueden elegir realizar una de las dos exploraciones solamente. \n",
    "\n",
    "* Profundidad: Entrene un par de redes con más capas LSTM y grafique como se comporta el desempeño a medida se aumenta la profundidad\n",
    "* Bidireccional: Varie la cantidad de unidades (a lo menos 4 valores distintos) de la capa LSTM y compare con la red utilizando capas LSTM bidireccionales (como muestra el código). Comente sobre el número de parámetros en ambos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5fmk3wJlVbZ"
   },
   "outputs": [],
   "source": [
    "def customUnits(value):\n",
    "  for i in range(value):\n",
    "    units = Sequential()\n",
    "\n",
    "    embedding_dim = 100\n",
    "    units.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "    units.add(LSTM(units=2**(i+7),return_sequences=True)) # You can use CuDNNLSTM if you have a CUDA enabled GPU for faster performance\n",
    "\n",
    "    units.add(Dense(n_tags, activation='softmax'))\n",
    "\n",
    "    units.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    units.summary()\n",
    "    history = units.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)\n",
    "    test_pred = units.predict(x_val, verbose=1)   \n",
    "    pred_labels = pred2label(test_pred)\n",
    "    test_labels = pred2label(y_val)\n",
    "    print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
    "    report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pT3o4G53qgH1",
    "outputId": "a4b52278-77d4-4222-8b3c-457a98ab4859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 140, 128)          117248    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 2,143,470\n",
      "Trainable params: 2,143,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 61s 2ms/step - loss: 0.3848 - acc: 0.9318 - val_loss: 0.1565 - val_acc: 0.9672\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.1096 - acc: 0.9715 - val_loss: 0.0781 - val_acc: 0.9788\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0609 - acc: 0.9834 - val_loss: 0.0512 - val_acc: 0.9863\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0418 - acc: 0.9887 - val_loss: 0.0419 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0344 - acc: 0.9901 - val_loss: 0.0394 - val_acc: 0.9886\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0305 - acc: 0.9909 - val_loss: 0.0378 - val_acc: 0.9887\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0282 - acc: 0.9914 - val_loss: 0.0365 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0364 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0250 - acc: 0.9921 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0238 - acc: 0.9924 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "8795/8795 [==============================] - 26s 3ms/step\n",
      "F1-score: 94.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.92      0.80      0.85      5000\n",
      "       B-eve       0.00      0.00      0.00        66\n",
      "       B-geo       0.97      0.99      0.98    220977\n",
      "       B-gpe       0.80      0.83      0.82      9380\n",
      "       B-nat       0.78      0.60      0.68      1494\n",
      "       B-org       0.75      0.72      0.73      1828\n",
      "       B-per       0.93      0.91      0.92      4145\n",
      "       B-tim       0.75      0.44      0.55      4104\n",
      "       I-art       0.00      0.00      0.00       100\n",
      "       I-eve       0.78      0.15      0.25        96\n",
      "       I-geo       0.82      0.72      0.76      4058\n",
      "       I-gpe       0.85      0.83      0.84      4134\n",
      "       I-nat       0.00      0.00      0.00        83\n",
      "       I-org       0.72      0.47      0.57      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.00      0.00      0.00        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.59      0.50      0.53   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 140, 256)          365568    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 140, 18)           4626      \n",
      "=================================================================\n",
      "Total params: 2,394,094\n",
      "Trainable params: 2,394,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 62s 2ms/step - loss: 0.3422 - acc: 0.9376 - val_loss: 0.1407 - val_acc: 0.9675\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 61s 2ms/step - loss: 0.0903 - acc: 0.9753 - val_loss: 0.0633 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0495 - acc: 0.9867 - val_loss: 0.0441 - val_acc: 0.9880\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0358 - acc: 0.9898 - val_loss: 0.0398 - val_acc: 0.9886\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0310 - acc: 0.9907 - val_loss: 0.0368 - val_acc: 0.9892\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0280 - acc: 0.9913 - val_loss: 0.0376 - val_acc: 0.9885\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0363 - val_acc: 0.9891\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0231 - acc: 0.9925 - val_loss: 0.0365 - val_acc: 0.9895\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 60s 2ms/step - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "8795/8795 [==============================] - 25s 3ms/step\n",
      "F1-score: 94.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.95      0.77      0.85      5000\n",
      "       B-eve       0.09      0.09      0.09        66\n",
      "       B-geo       0.97      0.99      0.98    220977\n",
      "       B-gpe       0.82      0.80      0.81      9380\n",
      "       B-nat       0.80      0.59      0.68      1494\n",
      "       B-org       0.77      0.71      0.74      1828\n",
      "       B-per       0.92      0.91      0.91      4145\n",
      "       B-tim       0.66      0.54      0.59      4104\n",
      "       I-art       0.00      0.00      0.00       100\n",
      "       I-eve       0.78      0.15      0.25        96\n",
      "       I-geo       0.80      0.76      0.78      4058\n",
      "       I-gpe       0.87      0.83      0.85      4134\n",
      "       I-nat       0.00      0.00      0.00        83\n",
      "       I-org       0.64      0.53      0.58      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       1.00      0.44      0.61        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.65      0.53      0.57   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 140, 512)          1255424   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 140, 18)           9234      \n",
      "=================================================================\n",
      "Total params: 3,288,558\n",
      "Trainable params: 3,288,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 61s 2ms/step - loss: 0.2794 - acc: 0.9444 - val_loss: 0.1180 - val_acc: 0.9698\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0760 - acc: 0.9783 - val_loss: 0.0578 - val_acc: 0.9851\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0433 - acc: 0.9881 - val_loss: 0.0415 - val_acc: 0.9885\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0333 - acc: 0.9902 - val_loss: 0.0385 - val_acc: 0.9890\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 58s 2ms/step - loss: 0.0291 - acc: 0.9911 - val_loss: 0.0389 - val_acc: 0.9885\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0268 - acc: 0.9916 - val_loss: 0.0369 - val_acc: 0.9895\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 58s 2ms/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0371 - val_acc: 0.9892\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 58s 2ms/step - loss: 0.0235 - acc: 0.9924 - val_loss: 0.0384 - val_acc: 0.9894\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 59s 2ms/step - loss: 0.0223 - acc: 0.9927 - val_loss: 0.0369 - val_acc: 0.9892\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 58s 2ms/step - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0377 - val_acc: 0.9896\n",
      "8795/8795 [==============================] - 26s 3ms/step\n",
      "F1-score: 94.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.91      0.80      0.85      5000\n",
      "       B-eve       0.68      0.20      0.31        66\n",
      "       B-geo       0.97      0.99      0.98    220977\n",
      "       B-gpe       0.81      0.82      0.82      9380\n",
      "       B-nat       0.81      0.59      0.68      1494\n",
      "       B-org       0.83      0.66      0.74      1828\n",
      "       B-per       0.90      0.91      0.91      4145\n",
      "       B-tim       0.64      0.57      0.60      4104\n",
      "       I-art       0.00      0.00      0.00       100\n",
      "       I-eve       0.80      0.17      0.28        96\n",
      "       I-geo       0.79      0.76      0.77      4058\n",
      "       I-gpe       0.80      0.90      0.85      4134\n",
      "       I-nat       0.29      0.02      0.04        83\n",
      "       I-org       0.75      0.45      0.56      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.81      0.44      0.57        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.69      0.55      0.59   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 140, 1024)         4608000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 140, 18)           18450     \n",
      "=================================================================\n",
      "Total params: 6,650,350\n",
      "Trainable params: 6,650,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 66s 2ms/step - loss: 0.3293 - acc: 0.9166 - val_loss: 0.0988 - val_acc: 0.9744\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 63s 2ms/step - loss: 0.0781 - acc: 0.9784 - val_loss: 0.0666 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 63s 2ms/step - loss: 0.0527 - acc: 0.9859 - val_loss: 0.0496 - val_acc: 0.9864\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 63s 2ms/step - loss: 0.0395 - acc: 0.9891 - val_loss: 0.0414 - val_acc: 0.9880\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 64s 2ms/step - loss: 0.0327 - acc: 0.9903 - val_loss: 0.0382 - val_acc: 0.9887\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 64s 2ms/step - loss: 0.0290 - acc: 0.9910 - val_loss: 0.0373 - val_acc: 0.9892\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 63s 2ms/step - loss: 0.0270 - acc: 0.9915 - val_loss: 0.0361 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 64s 2ms/step - loss: 0.0254 - acc: 0.9918 - val_loss: 0.0365 - val_acc: 0.9890\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 63s 2ms/step - loss: 0.0242 - acc: 0.9921 - val_loss: 0.0358 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 64s 2ms/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0368 - val_acc: 0.9895\n",
      "8795/8795 [==============================] - 25s 3ms/step\n",
      "F1-score: 94.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.92      0.80      0.86      5000\n",
      "       B-eve       0.06      0.12      0.08        66\n",
      "       B-geo       0.97      0.99      0.98    220977\n",
      "       B-gpe       0.81      0.82      0.81      9380\n",
      "       B-nat       0.86      0.55      0.67      1494\n",
      "       B-org       0.82      0.67      0.73      1828\n",
      "       B-per       0.93      0.91      0.92      4145\n",
      "       B-tim       0.69      0.48      0.56      4104\n",
      "       I-art       0.00      0.00      0.00       100\n",
      "       I-eve       0.72      0.14      0.23        96\n",
      "       I-geo       0.83      0.73      0.78      4058\n",
      "       I-gpe       0.87      0.82      0.85      4134\n",
      "       I-nat       0.25      0.02      0.04        83\n",
      "       I-org       0.64      0.52      0.57      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.86      0.52      0.65        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.66      0.53      0.57   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customUnits(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "64G_KilTUcoW",
    "outputId": "8cb16edb-6189-45a3-f018-07604be589e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 140, 100)          2023900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 140, 256)          234496    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 140, 18)           4626      \n",
      "=================================================================\n",
      "Total params: 2,263,022\n",
      "Trainable params: 2,263,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 116s 4ms/step - loss: 0.3166 - acc: 0.9393 - val_loss: 0.1184 - val_acc: 0.9683\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 114s 4ms/step - loss: 0.0850 - acc: 0.9759 - val_loss: 0.0628 - val_acc: 0.9830\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 114s 4ms/step - loss: 0.0482 - acc: 0.9868 - val_loss: 0.0450 - val_acc: 0.9876\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 114s 4ms/step - loss: 0.0342 - acc: 0.9905 - val_loss: 0.0378 - val_acc: 0.9895\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 114s 4ms/step - loss: 0.0280 - acc: 0.9919 - val_loss: 0.0360 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 113s 4ms/step - loss: 0.0243 - acc: 0.9928 - val_loss: 0.0351 - val_acc: 0.9902\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 114s 4ms/step - loss: 0.0219 - acc: 0.9934 - val_loss: 0.0354 - val_acc: 0.9900\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 113s 4ms/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0351 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 113s 4ms/step - loss: 0.0182 - acc: 0.9944 - val_loss: 0.0350 - val_acc: 0.9903\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 113s 4ms/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0360 - val_acc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_dim = 100\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=128,return_sequences=True)))\n",
    "\n",
    "model.add(Dense(n_tags, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "vBGKb1nOouS0",
    "outputId": "27c084c8-1f41-480e-c754-de7f26977441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8795/8795 [==============================] - 52s 6ms/step\n",
      "F1-score: 95.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.88      0.88      0.88      5000\n",
      "       B-eve       0.61      0.17      0.26        66\n",
      "       B-geo       0.98      0.99      0.98    220977\n",
      "       B-gpe       0.83      0.84      0.83      9380\n",
      "       B-nat       0.80      0.65      0.72      1494\n",
      "       B-org       0.72      0.73      0.73      1828\n",
      "       B-per       0.95      0.90      0.92      4145\n",
      "       B-tim       0.60      0.60      0.60      4104\n",
      "       I-art       0.17      0.01      0.02       100\n",
      "       I-eve       0.69      0.11      0.20        96\n",
      "       I-geo       0.83      0.76      0.80      4058\n",
      "       I-gpe       0.88      0.76      0.82      4134\n",
      "       I-nat       0.00      0.00      0.00        83\n",
      "       I-org       0.68      0.62      0.65      5020\n",
      "       I-per       0.00      0.00      0.00        64\n",
      "       I-tim       0.93      0.27      0.42        48\n",
      "           O       1.00      1.00      1.00    970703\n",
      "\n",
      "    accuracy                           0.99   1231300\n",
      "   macro avg       0.68      0.55      0.58   1231300\n",
      "weighted avg       0.99      0.99      0.99   1231300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from  sklearn_crfsuite.metrics import flat_classification_report \n",
    "\n",
    "test_pred = model.predict(x_val, verbose=1)   \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = pred2label(y_val)\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
    "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVPeQaaxUcoZ"
   },
   "source": [
    "#### 1.h Dimensión del Embedding\n",
    "\n",
    "Como podrán haber notado, gran parte de los parámetros entrenables se encuentran en el embedding. Elija una de las redes entrenadas anteriormente y varíe la dimensión del embedding en un conjunto de potencias de 2 que le parezca razonable (a lo menos 6 valores). Comente sus observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aKqe5nMUcoa"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def customEmbedding(values):\n",
    "  val_acc_max = [] # maximo accuracy de validacion obtenido\n",
    "  acc_epoch = []    # accuracy de training obtenido en el mismo epoch que val_acc_max\n",
    "\n",
    "  for i in range(values):\n",
    "    conv = Sequential()\n",
    "\n",
    "    embedding_dim = 2**(i+5)\n",
    "    conv.add(Embedding(input_dim=n_lemmas, output_dim=embedding_dim, input_length=max_len))\n",
    "    conv.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
    "    conv.add(Conv1D(128, 5, activation='relu', padding='same'))\n",
    "    conv.add(Dense(n_tags, activation=\"softmax\"))\n",
    "\n",
    "    conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = conv.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=0, batch_size=128)\n",
    "    conv.summary()\n",
    "    history = conv.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=10, batch_size=128)\n",
    "    val_acc_max.append(max(history.history['val_acc']))\n",
    "    acc_epoch.append(history.history['acc'][history.history['val_acc'].index(max(history.history['val_acc']))])\n",
    "  \n",
    "  plt.plot(val_acc_max,label=\"max val accuracy\")\n",
    "  plt.plot(acc_epoch,label=\"training accuracy\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('2^{embedding dimension + 5}')\n",
    "  plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p--kfatr0oXb",
    "outputId": "ae74d080-2c4b-49d5-8dda-2b340c6da708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 140, 32)           647648    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 140, 128)          20608     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 752,626\n",
      "Trainable params: 752,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 8s 318us/step - loss: 0.2873 - acc: 0.9614 - val_loss: 0.0789 - val_acc: 0.9762\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0606 - acc: 0.9827 - val_loss: 0.0481 - val_acc: 0.9874\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0357 - acc: 0.9902 - val_loss: 0.0360 - val_acc: 0.9898\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0260 - acc: 0.9924 - val_loss: 0.0333 - val_acc: 0.9906\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 2s 67us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0333 - val_acc: 0.9906\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0187 - acc: 0.9943 - val_loss: 0.0336 - val_acc: 0.9905\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 2s 69us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0353 - val_acc: 0.9907\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0148 - acc: 0.9954 - val_loss: 0.0358 - val_acc: 0.9904\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 2s 68us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0378 - val_acc: 0.9904\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 2s 69us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0387 - val_acc: 0.9903\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 140, 64)           1295296   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 140, 128)          41088     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 1,420,754\n",
      "Trainable params: 1,420,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 4s 150us/step - loss: 0.2614 - acc: 0.9651 - val_loss: 0.0675 - val_acc: 0.9796\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0487 - acc: 0.9862 - val_loss: 0.0379 - val_acc: 0.9895\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0282 - acc: 0.9918 - val_loss: 0.0325 - val_acc: 0.9906\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0219 - acc: 0.9935 - val_loss: 0.0324 - val_acc: 0.9906\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0184 - acc: 0.9943 - val_loss: 0.0331 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0336 - val_acc: 0.9907\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 2s 76us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0350 - val_acc: 0.9905\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 2s 75us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0378 - val_acc: 0.9904\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 2s 77us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0385 - val_acc: 0.9903\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 2s 78us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0408 - val_acc: 0.9897\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 140, 128)          2590592   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 2,757,010\n",
      "Trainable params: 2,757,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 4s 166us/step - loss: 0.2237 - acc: 0.9657 - val_loss: 0.0612 - val_acc: 0.9817\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 2s 86us/step - loss: 0.0413 - acc: 0.9884 - val_loss: 0.0346 - val_acc: 0.9902\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 2s 84us/step - loss: 0.0255 - acc: 0.9925 - val_loss: 0.0321 - val_acc: 0.9905\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 2s 85us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0327 - val_acc: 0.9907\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 2s 85us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0332 - val_acc: 0.9906\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 2s 84us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0345 - val_acc: 0.9906\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 2s 85us/step - loss: 0.0119 - acc: 0.9962 - val_loss: 0.0362 - val_acc: 0.9903\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 2s 84us/step - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0380 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 2s 84us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0405 - val_acc: 0.9899\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 2s 85us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0433 - val_acc: 0.9902\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 140, 256)          5181184   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 140, 128)          163968    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 5,429,522\n",
      "Trainable params: 5,429,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 5s 191us/step - loss: 0.1939 - acc: 0.9712 - val_loss: 0.0491 - val_acc: 0.9866\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0341 - acc: 0.9902 - val_loss: 0.0321 - val_acc: 0.9908\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0227 - acc: 0.9931 - val_loss: 0.0311 - val_acc: 0.9910\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0323 - val_acc: 0.9909\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 3s 104us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0334 - val_acc: 0.9906\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0358 - val_acc: 0.9906\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0384 - val_acc: 0.9905\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0404 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 3s 105us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0439 - val_acc: 0.9902\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 3s 104us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0466 - val_acc: 0.9900\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 140, 512)          10362368  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 140, 128)          327808    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 10,774,546\n",
      "Trainable params: 10,774,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 6s 231us/step - loss: 0.1637 - acc: 0.9706 - val_loss: 0.0391 - val_acc: 0.9892\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0294 - acc: 0.9914 - val_loss: 0.0314 - val_acc: 0.9908\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 4s 138us/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.0311 - val_acc: 0.9910\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0328 - val_acc: 0.9906\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0342 - val_acc: 0.9906\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0361 - val_acc: 0.9906\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0391 - val_acc: 0.9905\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0430 - val_acc: 0.9905\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0450 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 4s 137us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0485 - val_acc: 0.9901\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 140, 1024)         20724736  \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 140, 128)          655488    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 140, 128)          82048     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 140, 18)           2322      \n",
      "=================================================================\n",
      "Total params: 21,464,594\n",
      "Trainable params: 21,464,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26382 samples, validate on 8795 samples\n",
      "Epoch 1/10\n",
      "26382/26382 [==============================] - 8s 308us/step - loss: 0.1505 - acc: 0.9730 - val_loss: 0.0359 - val_acc: 0.9899\n",
      "Epoch 2/10\n",
      "26382/26382 [==============================] - 5s 203us/step - loss: 0.0273 - acc: 0.9919 - val_loss: 0.0313 - val_acc: 0.9909\n",
      "Epoch 3/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0311 - val_acc: 0.9910\n",
      "Epoch 4/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0327 - val_acc: 0.9909\n",
      "Epoch 5/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0357 - val_acc: 0.9908\n",
      "Epoch 6/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0376 - val_acc: 0.9904\n",
      "Epoch 7/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0426 - val_acc: 0.9904\n",
      "Epoch 8/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0444 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "26382/26382 [==============================] - 5s 203us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0479 - val_acc: 0.9902\n",
      "Epoch 10/10\n",
      "26382/26382 [==============================] - 5s 202us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0519 - val_acc: 0.9902\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEHCAYAAAC5u6FsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSCxxGGOMCYklDmOMMSGxxGGMMSYk/x/tr0jbh0ecvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "customEmbedding(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZNHM0liUcoe"
   },
   "source": [
    "#### 1.i Escribamos palabras - Datasest\n",
    "\n",
    "Ahora buscaremos otra aplicación a las redes recurrentes, predecir el caracter siguiente. Si logramos entrenar una red que sea buena en esta tarea, podremos escribir texto automáticamente, pues podemos, a partir de una frase, predecir el caracter siguiente, y luego introducir la nueva frase sin el primer caracter en la red nuevamente, e iterando así escribir automáticamente. Si bien las redes recurrentes son adecuadas para esta tarea, no pretendemos entrenar un _Shakespeare_ en esta tarea, sin embargo es interesante investigar qué tan verosimil o no puede lograr ser el texto generado. \n",
    "\n",
    "Para esto, primero crearmos nuestro nuevo dataset. Para esta tarea preferiremos unir todas las frases en un solo gran corpus y luego crear nuevas secuencias semi redundantes. Esto nos evita primero el problema de tener que hacer padding, pues crearemos todas las entradas iguales, pero también nos permite aprovechar mejormente el dataset, de cierta forma aumentando el número de datos. El _target_ en este caso será solo el caracter siguiente correspondiente a cada secuencia. \n",
    "\n",
    "En este item debe cargar el dataset, completando el código propuesto abajo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09TxuGb5Ucof"
   },
   "outputs": [],
   "source": [
    "df_w = pd.read_csv(os.path.join(\"data\",\"entity-annotated-corpus\",\"ner.csv\"), engine='python', error_bad_lines=False)\n",
    "\n",
    "df_w = df_w.dropna()[['word']]\n",
    "\n",
    "corpus = ' '.join(list(df_w.word.values)).lower()\n",
    "\n",
    "sentence_length = 40\n",
    "steps = 5\n",
    "\n",
    "sentences = []\n",
    "next_char = []\n",
    "for i in range(0,len(corpus) - sentence_length - 1 , steps):\n",
    "    sentences.append( # . . . )\n",
    "    next_char.append( # . . . )\n",
    "\n",
    "chars_to_code = {char:code for code, char in enumerate(set(corpus))}\n",
    "code_to_chars = {code:char for char,code in chars_to_code.items()}\n",
    "\n",
    "x = pd.np.array([[chars_to_code[char] for char in sentence] for sentence in sentences])\n",
    "# . . .\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIOuoDybUcoi"
   },
   "source": [
    "#### 1.j Escribamos palabras - Red\n",
    "\n",
    "Entrene ahora una red con estos datos utilizando GRU. El resto de la estructura queda a su elección. Evalue el desempeño de su red evaluando qué tan bien genera texto, puede utilizar las funciones propuestas como `callback` para ver como progresa su red. Pruebe a lo menos 2 estructuras distintas. \n",
    "\n",
    "Una vez esté satisfecho de su red, hagala escribir algunos textos a partir de textos semilla elegidos por usted. Describa sus observaciones. ¿Qué cree ocurriría si entrenamos la red con otro dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qcqCMEfUcoj"
   },
   "outputs": [],
   "source": [
    "def predict_char(model, sentence):\n",
    "    x = [chars_to_code[char] for char in sentence]\n",
    "    x = pad_sequences([x], maxlen=sentence_length, padding='pre', value=0)\n",
    "    probas = model.predict(x)[0]\n",
    "    next_index = np.random.choice(len(chars_to_code), p=probas)\n",
    "    return code_to_chars[next_index]\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print(f'\\n Generating random text for epoch: {epoch}')\n",
    "    start_index = random.randint(0,x.shape[0]-1)\n",
    "    sentence = ''.join([code_to_chars[code] for code in x[start_index]])\n",
    "    print('\\n Generating with seed: ' + sentence)\n",
    "    sys.stdout.write(sentence)\n",
    "    for i in range(400):\n",
    "        next_char = predict_char(char, sentence)\n",
    "        sentence = sentence[1:] + next_char #for next character\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    return\n",
    "\n",
    "print_text_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# . . . \n",
    "\n",
    "character.fit(x,y, epochs=35, callbacks=[print_text_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gtrgBG1Ucom"
   },
   "source": [
    "# 2 - Autoencoders en Fashion MNIST\n",
    "\n",
    "Si bien las redes neuronales han tenido desempeños sorprendentes en muchas áreas donde antes solo un ser humano podía alcanzar buenos desempeños, uno de sus desventajas suele serla alta dimensionalidad de los espacios de hipótesis. En la práctica, esto implica que para aprender una tarea predictiva con alguna capacidad de generalización, se requieren grandes bases de datos etiquetadas. Esto implica un problema, considerando que al momento de buscar la base de datos, no se tiene a priori una manera de automatizar esta etiquetación. Esta necesidad de gran cantidad de trabajo de clasificación realizado por humanos, ha engendrado soluciones ingeniosas, como la aproximación de _Facebook_ hace algunos años de pedirle a los mismos usuarios que etiquetaran a las personas en sus fotos, o la solución de _Amazon_, _Mechanical Turk_, donde cualquier usuario puede realizar tareas repetitivas de clasificación a cambio de dinero real, o por otro lado cualquier persona puede comprar la etiquetación de una base de datos la cual realizan varias personas en cualquier parte del mundo. \n",
    "\n",
    "Otra aproximación, quizás aún más ingeniosa, para solucionar el problema de las etiquetas, es utilizar las bases de datos sin preocuparse de sus etiquetas. Esta aproximación de aprendizaje no supervisado tiene su representante en redes neuronales en los _Autoencoders_, redes que utilizan el mismo input como target y buscan representaciones de menor dimensionalidad al interior de la red. Estas redes han permitido el uso de cantidades masivas de datos para aprender de ellos sin necesidad de tener etiquetas. Durante esta pregunta veremos algunos de los aspectos y posibilidades básicas que nos presentan los _autoencoders_, utilizando una base de datos de imagenes de articulos de vestimenta, el Fashion MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlnEwUxeUcon"
   },
   "source": [
    "### 2.a Carga de datos y visualizaciones\n",
    "Cargue los datos. Puede user las funciones de `keras.datasets` como muestra el código o descargarlo manualmente. \n",
    "\n",
    "Luego, visualice algunas imagenes de cada una de las catégorias junto con sus nombres (investigue un poco para encontrar la codificación de `y`). Note que las imagenes deben representarse en blanco y negro, puede usar `cmap='Greys'`. ¿Qué pares de categorías cree podrían ocasionar problemas al momento de clasificación? ¿Qué tan bien cree que se desempeñaría un humano en esta tarea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ITS6QlZUcoo"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train,y_train),(x_val,y_val) = fashion_mnist.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(# . . .)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trEOE3oDUcos"
   },
   "source": [
    "### 2.b Posibilidades de preprocesamiento y pequeños análisis. \n",
    "\n",
    "¿Cuáles son los rangos de valores de `x`? ¿Por qué?\n",
    "\n",
    "¿Las distintas clases de ejemplos están balanceadas?\n",
    "\n",
    "¿Considera necesario realizar un preprocesamiento? Escale los valores de `x` al intervalo $[0,1]$, y guarde el conjunto de datos original de igual manera que el escalado. ¿Se pierde información al realizar este preprocesamiento? \n",
    "\n",
    "Las primeras redes que entrenaremos utilizarán arquitecturas _fully connected_, por lo cual también es necesario transformar nuestras imagenes 2-dimensionales a vectores, como muestra el ejemplo de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oXV2oRkUcot"
   },
   "outputs": [],
   "source": [
    "# . . . \n",
    "x_train_vector = x_train.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeXwnzvjUcox"
   },
   "source": [
    "### 2.c Primer Autoencoder\n",
    "\n",
    "Entrenaremos un primer autoencoder de una capa oculta, usando arquitectura densa. Para esto, utilize como guía los códigos presentados abajo. \n",
    "\n",
    "Utilice en primera instancia su conjunto de datos escalados. Considerando el intervalo de los datos escalados, ¿Qué función de activación correspondería a la capa de salida de la red? ¿Debería afectar la elección de la función de activación de la capa oculta? \n",
    "\n",
    "Entrene esta primera red utilizando pérdida _binary cross entropy_. Compare luego las imagenes originales con las imagenes reconstruidas, como muestra el código. ¿Qué le parece el desempeño de la red, logra aprender la tarea en su opinion? Grafique como varia la pérdida a lo largo del entrenamiento y visualice algunas imagenes reconstruidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zci6J8pRUcoy"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dense(32,activation='relu')) # encoder\n",
    " \n",
    "autoencoder.add(Dense(28*28,activation= # . . . )) #decoder\n",
    "\n",
    "autoencoder.compile(# . . .\n",
    "    \n",
    "autoencoder.fit(x_tr,x_tr,epochs=50,validation_data=(x_val,x_val) # . . .\n",
    "    \n",
    "                \n",
    "plt.imshow(autoencoder.predict(x_val[i]).reshape(28,28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbDWiAkQUco0"
   },
   "source": [
    "### 2.d Sin escalar\n",
    "Entrene nuevamente la misma estructura de red pero esta vez utilizando los datos originales (sin escalar). ¿Qué función de activación debe utilizar para la capa de salida? ¿Le parece sea una buena aproximación comparando con las caracteristicas de la red anterior? \n",
    "\n",
    "Compare los desempeños viendo las imagenes reconstruidas. \n",
    "\n",
    "De aquí en adelante prefiera la versión de los datos escaladas para consistencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scK-wEelUco1"
   },
   "outputs": [],
   "source": [
    "# do it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Wu-bIcqUco4"
   },
   "source": [
    "### 2.e Dimensionalidad\n",
    "\n",
    "Una forma de interpretar lo que realiza el autoencoder, es considerar que si el autoencoder hace bien su tarea, la información necesaria para reconstruir la imagen original se encuentra en la capa oculta, la cual tiene menor dimensionalidad que la imagen original. Uno puede considerar por lo tanto que la capa de _encoding_ esta comprimiendo la información contenida en la imagen, mientras la capa de _decoding_ hace el proceso contrario, descomprimiendola a su estado original lo mejor posible. \n",
    "\n",
    "Explore como cambia el desempeño de la red en cuestión frente a cambios en la dimensión de la capa oculta. Pruebe a lo menos 5 niveles de compresión distintos, incluyendo uno donde la capa oculta tenga $50\\%$ de ratio de compresión y otro donde la capa oculta tenga tan solo 2 neuronas. ¿Qué observa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ubk0K285Uco5"
   },
   "outputs": [],
   "source": [
    "# do it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_qX1lXDUco8"
   },
   "source": [
    "### 2.f Deep autoencoder\n",
    "\n",
    "Pruebe ahora con una arquitectura ligeramente más profunda. Para esto utilice a lo menos 3 capas de encoding, es decir, 3 capas que progresivamente reduzcan la dimensionalidad de la representación hasta una dimensión objetivo inicialmente igual a la mejor obtenida en la pregunta anterior. Utilice igualmente a lo menos 2 capas de decoding, que se encarguen de aumentar la dimensionaliad de la representación hasta alcanzar la dimensión de la imagen original. Note que las primeras capas no necesariamente deben tener menor dimensionalidad que la imagen, la dimensión relevante es aquella de la última capa de encoding. \n",
    "\n",
    "¿Cómo aumenta el número de parámetros entrenables? ¿Aumenta el tamaño de la representación \"comprimida\"?\n",
    "\n",
    "Una vez esté satisfecho con su arquitectura profunda, varíe la dimensión objetivo de la última capa de encoding, realizando una exploración similar a la pregunta anterior. \n",
    "\n",
    "¿Puede obtener una representación de menor dimensionalidad que la encontrada en el item anterior sin perder calidad en las imagenes obtenidas?\n",
    "\n",
    "Utilice gráficos y muestre algunas imágenes reconstruidas para complementar sus comentarios. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DEvuyZuUco9"
   },
   "outputs": [],
   "source": [
    "# do it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFrRThT_UcpA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Re3OmZPDUcpC"
   },
   "source": [
    "### 2.g Convolutional Autoencoder\n",
    "\n",
    "Como hemos hasta ahora  utilizado una arquitectura fully connected, nuestra red no toma en cuenta la infomación local contenida en la proximidad de un pixel en la imagen. Como vimos en la tarea anterior, esta información podría ser crucial al momento de procesar imágenes. \n",
    "\n",
    "En este item deberá implementar un autoencoder convolucional. La sección de encoding de la red se creará de igual manera que las redes convolucionales creadas en la tarea 1. Puede utilizar capas de Max Pooling o Strides mayores a 1 para reducir la dimensionalidad en esta etapa. Tenga en mente como varía la dimensión de la imagen a lo largo del proceso. \n",
    "\n",
    "Para luego recuperar la dimensionalidad de la imagen original debemos utilizar una capa llamada usualmente como \"Deconvolution Layer\". Esta capa realiza el proceso inverso que aquel realizado por una capa convolucional, por lo cual utilizando por ejemplo `stride=2` puede duplicar la dimensionalidad de su input. \n",
    "\n",
    "Puede realizar la profundidad que desee, pero tome en cuenta que una mayor profundidad de la sección convolucional permite a la red reducir dimensionalidad más lentamente, sin \"forzar\" la compresión de las características. Note que para reconstruir la dimensión original puede usar `output_padding` para corregir problemas de paridad, entre otros. \n",
    "\n",
    "Puede igualmente optar por usar algun número de capas densas en el cuello de botella del autoencoder, usando al comienzo de esta una capa `Flatten` y al final de ella una capa `Reshape` (`keras.layers.Reshape(target_shape)`) para recuperar la bidimensionalidad.\n",
    "\n",
    "¡No olvide que para entrenar esta red debe usar la versión bidimensional de los datos escalados!\n",
    "\n",
    "Visualice que tan bien se comporta la convolución, en terminos de la función de pérdida y visualizando las imagenes reconstruidas. Compárese con la red densa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7hX-Au0UcpD"
   },
   "outputs": [],
   "source": [
    "autoencoder_conv.add(Deconvolution2D(64, 2, strides=2,activation='relu', padding='same'))\n",
    "autoencoder_conv.add(Deconvolution2D(1, 2, strides=2,activation='sigmoid', padding='same'))\n",
    "\n",
    "# . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gv7NlnBUcpF"
   },
   "source": [
    "### 2.h Transfer Learning y reducción de dimensionalidad\n",
    "\n",
    "Como mencionamos inicialmente, una de las utilidades de los autoencoders corresponde a la no necesidad de tener etiquetas. En ese caso, podemos utilizar los autoencoders para extraer representaciones significativas de los ejemplos no etiquetados, y luego utilizar ese conocimiento para mejorar el desempeño de nuestros algoritmos. \n",
    "\n",
    "Para simular esta tarea supongamos que tenemos la misma base de datos original, sin embargo solo poseemos un pequeño porcentaje de las etiquetas del conjunto de entrenamiento, menos del $5\\%$ (considere que si tuviera que etiquetar manualmente incluso solo un $5\\%$ correspondería a 3000 imagenes por etiquetar!).\n",
    "\n",
    "Seleccione un subconjunto de datos correspondiente a menos del $5\\%$ de los datos y entrene un modelo predictivo para el problema de clasificación. Si bien la aproximación convolucional suele ser la mejor para este tipo de bases de datos, utilice arquitectura densa. \n",
    "\n",
    "Luego, utilizando el mejor autoencoder entrenado hasta ahora (o entrene uno nuevo si prefiere), utilize lo aprendido por la red sobre el conjunto de datos sin etiquetas para intentar aprender la relación suyaciente. Para esto, lo más fácil es primero preprocesar el conjunto `x` de entrenamiento, utilizando la sección de encoding de su autoencoder. Puede crear un nuevo modelo secuencial solo con las capas de encoding como se muestra en el código y luego usar el método `.predict` del modelo. Luego sobre estos datos preprocesados entrene un nuevo modelo predictivo fully connected. \n",
    "\n",
    "La otra opción sería crear un modelo cuya primeras capas correspondan a la sección de encoding del autoencoder y las últimas capas a su modelo denso, pero fijando el atributo `trainable` de las primeras capas como `False`, sin embargo esto resulta poco eficiente. \n",
    "\n",
    "\n",
    "Comente sus resultados basándose en métricas adecuadas. Comente igualmente sobre la dimensionalidad de las representaciones utilizadas por cada uno de los dos modelos entrenados en esta sección y el número de parametros _entrenables_ y totales del modelo (considerando el encoding). \n",
    "\n",
    "Considerando la dimensionalidad obtenida por el encoder, compare la calidad de la representación obtenida por el autoencoder frente a otras aproximaciones de reducción de dimensionalidad, como por ejemplo Principal Component Analysis (PCA). Para esto, transforme los datos de entrenamiento utilizando PCA con un número de componentes principales igual a la dimensión de la representación engendrada por su encoder. Compare el desempeño de un modelo determinado utilizando ambas representaciones. \n",
    "\n",
    "Comente sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGZ3gOtnUcpG"
   },
   "outputs": [],
   "source": [
    "encoder = Sequential()\n",
    "\n",
    "encoder.add(best_autoencoder.get_layer(index=0))\n",
    "\n",
    "# . . . add as many as you need\n",
    "\n",
    "encoder.predict( # . . .\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=d)\n",
    "\n",
    "# . . . fit and transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqVHsBUTUcpI"
   },
   "source": [
    "### 2.i Denoising Autoencoder \n",
    "\n",
    "Otra utilidad que se le ha dado a los autoencoders es la posibilidad de utilizarlos para separar ruido de información. Para entrenar tal tipo de modelo, la idea es simple: utilizar como datos de entrada imagenes a las cuales se les ha agregado artificialmente ruido y como objetivo la imagen original sin ruido. \n",
    "\n",
    "Entrene alguna arquitectura de autoencoder que le parezca apropiada para la tarea utilizando algún tipo de ruido aleatorio. Puede utilizar cambios en valores de pixeles aleatoriamente, o por ejemplo \"promediar\" ponderadamente la imagen original con alguna otra imagen del dataset ligeramente modificada. Puede utilizar las librerías `random` de `numpy` o utilizar por ejemplo el `ImageDataGenerator` que utilizamos en la tarea anterior para generar imágenes similares con las cuales \"sumar\" ruido. Sea creativo, puede crear el ruido que desee. Idealmente, considerando la naturaleza del problema que se buscaría modelar (eliminar ruido real de mediciones), la naturaleza del ruido agregado debe ser estocástica y no puede \"repetirse\" el mismo patron de ruido a lo largo de todo el entrenamiendo, es decir, si agregó un ruido estocástico a cada imagen del conjunto, este proceso debe iterarse igualmente luego de cada época de entrenamiento, para evitar que la red aprenda un patrón especifico de ruido, si no aprenda realmente a diferenciar ruido sin información de la información suyaciente a la imagen. \n",
    "\n",
    "Una vez esté satisfecho con la red, muestre ejemplos de la imagen con ruido, la imagen original y la imagen reconstruida. Pruebe igualmente entregarle a la red nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADOwFwH8UcpJ"
   },
   "outputs": [],
   "source": [
    "from numpy.random import # . . .\n",
    "\n",
    "# . . .\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIlMRCsmUcpM"
   },
   "source": [
    "### 2.j Generación de imágenes\n",
    "\n",
    "Otra utilización posible que podría darse, quizás, a los autoencoders, es utilizar los decoders para generar nuevas imagenes. La idea de esto sería considerar que la habilidad que tiene la sección decoder de generar una imagen a partir de una representación de menor dimensionalidad puede aprovecharse. \n",
    "\n",
    "Para esto, extraiga la sección de encoding y la sección de decoding de alguno de los autoencoders entrenados que prefiera. Obtenga los valores de la representación interna de las imagenes usando el encoder para calcularlos. Luego agréguele a esta representación algún ruido de su preferencia, y calcule la imagen resultante utilizando el decoder. ¿Qué observa? Muestre las imagenes obtenidas junto con otras imagenes de la misma categoría que la imagen que utilizó originalmente. Preube con distintos valores de ruido.\n",
    "\n",
    "Pruebe también, por ejemplo, calcular la imagen obtenida al promediar las representaciones comprimidas de varias (o todas) las imagenes de una clase. Utilice el encoder para generar imagenes a partir de otros valores que se les ocurran y especule sobre el por qué la imagen obtenida se asimila o no a las imagenes del dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKM5QCbnUcpN"
   },
   "outputs": [],
   "source": [
    "# do it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4glRJBpdUcpR"
   },
   "source": [
    "# 3 - GAN para MNIST \n",
    "\n",
    "Probablemente uno de los desarrollos recientes del area de las redes neuronales más interesantes son las GAN, o _Generative Adversarial Networks_. Estas han deslumbrado al mundo los últimos años generando resultados inesperados, como los llamados _deep fakes_ (https://www.youtube.com/watch?v=dh-QM54RuAs), caras no no existentes generadas artificialmente, entre muchas otras aplicaciones de las cuales las más creativas y divertidas son ampliamente divulgadas. Estos resultados nos dan cuenta que estas redes cuando son implementadas correctamente tienen la habilidad de realizar tareas muy especificas logrando desempeños que en logran incluso en algunos casos engañar a observadores humanos.\n",
    "\n",
    "<img src=\"https://pathmind.com/images/wiki/GANs.png\" background=\"white\">\n",
    "\n",
    "Dependiendo del problema que uno quiera resolver las GAN no requieren datos etiquetados. Esto lo logran gracias a su estructura adversarial, es decir, donde simultaneamente se entrenan dos redes, una especializada en generar datos y otra red especializada en discriminar datos verdaderos de datos falsos, en su configuración más simple al menos. En esta parte de la tarea deberan generar una red de este tipo que genere dígitos que aparenten ser hechos a mano. Para esto utilizaremos las imagenes de entrenamiento del dataset MNIST que ya conocen.\n",
    "\n",
    "A diferencia de otras preguntas, en esta deberán tener mayor iniciativa propia y solamente se presentará una estructura básica la cual ustedes deberán completar y ajustar para que el entrenamiento se realice bien. Además deberán:\n",
    "\n",
    "* Graficar alguna medida del desempeño tanto del generador como del disciminador (por ejemplo f1-score, precision and recall o accuracy) a lo largo del entrenamiento. ¿Variaciones en esta medidas representan mejoras en nuestra red?\n",
    "* Visualizar imagenes generadas por el generador a lo largo del entrenamiento y ver su evolución (idealmente graficar epocas representativas de etapas del entrenamiento)\n",
    "* Explicar el comportamiento de la evolución de ambos desempeños y por qué no necesariamente esos valores representan que la red alcance su cometido\n",
    "* Describir, teórica o práctiamente, que ocurriría si la red generadora no pudiera por algún motivo (divergencia en entrenamiento, excesivo _underfitting_, etc) generar imágenes razonables.\n",
    "* Describir la contraparte de lo que ocurriría si la red discriminadora no pudiera aprender a diferenciar imagenes reales de ruido aleatorio.\n",
    "* Una vez teniendo una red entrenada a completitud, muestre varias imagenes generadas. ¿Se logra obtener imagenes convincentes?\n",
    "* Igualmente con la red entrenada, guarde el ruido aleatorio que origina dos números reconocibles distintos. ¿Qué ocurre si vemos las imagenes generadas por el generador al entregarle puntos distribuidos linealmente entre los dos puntos?\n",
    "* Proponga o investigue como realizaría las siguientes tareas:\n",
    "    * A partir de una base de datos de imagenes RGB, entrenar una GAN que genere imágenes a color convincentes a partir de imagenes en blanco y negro, es decir que deduzca el color a partir de imagenes en blanco y negro.\n",
    "    * A partir de una base de datos de cuadros de paisajes reales y las fotos correspondientes al cuadro, entrenar una GAN que permita transformar fotografias a cuadros y viceversa (puede utilizar más de 2 redes)\n",
    "    * Proponga un problema que les parezca interesante y una estructura de GAN que le permitiría resolverlo\n",
    "    \n",
    "Esta pregunta si bien es libre, debe ser redactada de manera ordenada. La recomendación es separar tanto el código como las respuestas en varios bloques, y enunciar nuevamente las preguntas o redactar sus respuestas de manera que no quede duda a qué pregunta en particular estan respondiendo. El código de abajo está incompleto y es solo una guía (de todas formas la estructura y entrenamiento de la red \"funciona\", aunque se recomienda prueben pequeños cambios), puede crear funciones, cambiar nombres e incluso las redes mismas como estimen conveniente, la intención es que se aproximen a la exploración que deberán hacer si alguna vez les toca resolver un problema real con ANN y luego reportar sus resultados. \n",
    "\n",
    "<!-- glhf -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-WRKn2rUcpR"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x,_),(_,_) = mnist.load_data()\n",
    "x = x.reshape((-1,28,28,1)).astype(np.float32)\n",
    "x = x/255\n",
    "\n",
    "\n",
    "generator_net = Sequential()\n",
    "\n",
    "generator_net.add(Dense(7*7*256, input_dim=100))\n",
    "generator_net.add(BatchNormalization(momentum=0.9))\n",
    "generator_net.add(LeakyReLU())\n",
    "generator_net.add(Reshape((7,7,256)))\n",
    "generator_net.add(Dropout(dropout_prob))\n",
    "\n",
    "generator_net.add(UpSampling2D())\n",
    "generator_net.add(Conv2D(128, 5, padding='same'))\n",
    "generator_net.add(BatchNormalization(momentum=0.9))\n",
    "generator_net.add(LeakyReLU())\n",
    "\n",
    "generator_net.add(UpSampling2D())\n",
    "generator_net.add(Conv2D(64, 5, padding='same'))\n",
    "generator_net.add(BatchNormalization(momentum=0.9))\n",
    "generator_net.add(LeakyReLU())\n",
    "\n",
    "generator_net.add(Conv2D(32, 5, padding='same'))\n",
    "generator_net.add(BatchNormalization(momentum=0.9))\n",
    "generator_net.add(LeakyReLU())\n",
    "\n",
    "generator_net.add(Conv2D(1, 5, padding='same'))\n",
    "generator_net.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "discriminator_net = Sequential()\n",
    "\n",
    "discriminator_net.add(Conv2D(64, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "discriminator_net.add(LeakyReLU())\n",
    "\n",
    "discriminator_net.add(Conv2D(128, 5, strides=2, padding='same'))\n",
    "discriminator_net.add(LeakyReLU())\n",
    "discriminator_net.add(Dropout(dropout_prob))\n",
    "\n",
    "discriminator_net.add(Conv2D(256, 5, strides=2, padding='same'))\n",
    "discriminator_net.add(LeakyReLU())\n",
    "discriminator_net.add(Dropout(dropout_prob))\n",
    "\n",
    "discriminator_net.add(Conv2D(512, 5, strides=1, padding='same'))\n",
    "discriminator_net.add(LeakyReLU())\n",
    "discriminator_net.add(Dropout(dropout_prob))\n",
    "\n",
    "discriminator_net.add(Flatten())\n",
    "discriminator_net.add(Dense(1))\n",
    "discriminator_net.add(Activation('sigmoid'))\n",
    "\n",
    "optimizer_discriminator = RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-10)\n",
    "discriminator = Sequential()\n",
    "discriminator.add(discriminator_net)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer = optimizer_discriminator, metrics=['accuracy'])\n",
    "\n",
    "optimizer_gan = Adam(lr=0.0004, clipvalue=1.0, decay=1e-10)\n",
    "gan = Sequential()\n",
    "gan.add(generator_net)\n",
    "for layer in discriminator_net.layers:\n",
    "    layer.trainable = False\n",
    "gan.add(discriminator_net)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer_gan, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "for i in range(total_epochs):\n",
    "    images_train = # select batch_size random images from x\n",
    "    \n",
    "    noise = # generate (batch_size,latent_dim) sized noise\n",
    "    images_fake = generator_net.predict(noise)\n",
    "\n",
    "    x_train = np.concatenate((images_train, images_fake))\n",
    "\n",
    "    response_dis = discriminator.train_on_batch(x_train, y)\n",
    "    response_gen = gan.train_on_batch(noise,y_gen)\n",
    "\n",
    "    if i%save_results_every==0:\n",
    "        # save accuracies and losses for plotting\n",
    "        \n",
    "        if i%plot_every==0:\n",
    "            # plot random generated images and losses and accuracies"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Enunciado Tarea 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
